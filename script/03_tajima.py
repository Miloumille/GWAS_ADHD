#!/usr/bin/env python3
import os
import sys
import subprocess
import csv
import glob # To find the VCF files
import allel # scikit-allel for calculations
import numpy as np # scikit-allel often uses numpy arrays

# === Configuration (Adapt if necessary!) ===

# Directory containing the VCF files generated by the previous script
VCF_DIR = "output_vcfs" # <--- Should match OUTPUT_DIR from previous script

# Local panel file (used to identify EUR samples)
PANEL_FILE_LOCAL = "data/integrated_call_samples_v3.20130502.ALL.panel.txt"

# Target super-population code
POPULATION_CODE = "SAS"

# Command for bcftools (Use the full path if needed)
# Make sure this path is correct
BCFTOOLS_CMD = "/opt/anaconda3/envs/bcftools/bin/bcftools" # <--- ADJUST IF NEEDED

# === Helper Functions ===

def get_samples_from_vcf(bcftools_cmd, vcf_path):
    """Extracts the list of sample IDs from a VCF file using bcftools."""
    command = [bcftools_cmd, "query", "-l", vcf_path]
    try:
        result = subprocess.run(command, check=True, capture_output=True, text=True)
        samples = set(result.stdout.strip().split('\n'))
        if not samples or (len(samples) == 1 and '' in samples):
            print(f"WARNING: No samples found or extracted from {vcf_path}")
            return set()
        # print(f"DEBUG: Found {len(samples)} samples in {vcf_path}")
        return samples
    except FileNotFoundError:
        print(f"ERROR: bcftools command '{bcftools_cmd}' not found.")
        print("  Please ensure bcftools is installed and the path is correct.")
        sys.exit(1)
    except subprocess.CalledProcessError as e:
        print(f"ERROR: bcftools failed to extract samples from '{vcf_path}'.")
        print(f"  Return Code: {e.returncode}")
        print(f"  Stderr: {e.stderr}")
        # Don't exit, just return empty set for this file
        return set()
    except Exception as e:
        print(f"ERROR: An unexpected error occurred extracting samples from '{vcf_path}': {e}")
        # Don't exit, just return empty set for this file
        return set()


def get_population_samples(panel_file, population_code):
    """Reads the panel file and returns a set of sample IDs for the target population."""
    pop_samples = set()
    print(f"Reading panel file '{panel_file}' to find {population_code} samples...")
    try:
        with open(panel_file, 'r') as infile:
            reader = csv.reader(infile, delimiter='\t')
            header = next(reader) # Skip header
            try:
                sample_col_idx = header.index('sample')
                pop_col_idx = header.index('super_pop')
            except ValueError as e:
                print(f"ERROR: Missing expected column in panel file header: {e}")
                print(f"Expected columns 'sample' and 'super_pop'. Found: {header}")
                sys.exit(1)

            for row in reader:
                 # Check row length before accessing indices
                 if len(row) > max(sample_col_idx, pop_col_idx) and row[pop_col_idx] == population_code:
                    pop_samples.add(row[sample_col_idx])

        print(f"Found {len(pop_samples)} samples for population {population_code} in panel file.")
        if not pop_samples:
             print(f"WARNING: No samples found for population {population_code} in the panel file.")
        return pop_samples

    except FileNotFoundError:
        print(f"ERROR: Panel file '{panel_file}' not found!")
        sys.exit(1)
    except Exception as e:
        print(f"ERROR: Failed to process panel file '{panel_file}': {e}")
        sys.exit(1)


def calculate_tajima_d(vcf_path, target_samples):
    """
    Reads VCF, subsets to target samples, and calculates Tajima's D.
    Returns the Tajima's D value or None if calculation fails.
    """
    print(f"Calculating Tajima's D for {len(target_samples)} target samples in {vcf_path}...")
    try:
        # Read VCF data using scikit-allel
        # Only need GT (genotypes), variants/POS (positions), samples
        callset = allel.read_vcf(vcf_path, fields=['samples', 'variants/POS', 'calldata/GT'])

        if callset is None:
            print("WARNING: No data returned from VCF file (possibly empty or no variants). Skipping.")
            return None

        # Get all samples listed in the VCF header
        vcf_samples = callset['samples']
        # print(f"DEBUG: Samples in VCF header: {len(vcf_samples)}")

        # Find the indices of our target samples within the VCF sample list
        target_indices = [i for i, s in enumerate(vcf_samples) if s in target_samples]

        if not target_indices:
            print("WARNING: None of the target population samples were found in this VCF file. Skipping.")
            return None

        # print(f"DEBUG: Found {len(target_indices)} target samples indices in VCF.")

        # Extract the genotype data for the target samples
        # Genotype array shape: (n_variants, n_samples, ploidy)
        gt = allel.GenotypeArray(callset['calldata/GT'])
        gt_subset = gt.take(target_indices, axis=1)

        # Check if there are variants after subsetting
        if gt_subset.shape[0] == 0:
            print("WARNING: No variant sites found in the VCF for the target samples. Skipping.")
            return None

        # Calculate allele counts for the subset
        # Allele count array shape: (n_variants, n_alleles) -> usually (n_variants, 2) for biallelic
        ac_subset = gt_subset.count_alleles()

        # Check if there is variation in the subset
        # is_variant = ac_subset.is_variant() # Check which sites are variant in the subset
        # Need at least two segregating sites for Tajima's D typically
        n_segregating_sites = np.sum(ac_subset.is_variant())
        if n_segregating_sites < 2:
             print(f"WARNING: Only {n_segregating_sites} segregating sites found for target samples. Cannot reliably calculate Tajima's D. Skipping.")
             return None

        # Calculate Tajima's D
        # We calculate it for the entire region defined by the VCF file
        # If the VCF contains multiple contigs/chromosomes, this might not be appropriate
        # We assume VCFs from the previous step are for single contiguous regions.
        pos = callset['variants/POS'] # Positions are needed for the calculation window context
        tajima_d_value = allel.tajima_d(ac_subset, pos=pos)

        # Handle potential NaN results (e.g., if no variation)
        if np.isnan(tajima_d_value):
            print("WARNING: Tajima's D calculation resulted in NaN (possibly no variation).")
            return None

        print(f"Tajima's D calculated: {tajima_d_value:.4f}")
        return tajima_d_value

    except FileNotFoundError:
        print(f"ERROR: VCF file '{vcf_path}' not found during Tajima's D calculation.")
        return None
    except Exception as e:
        print(f"ERROR: Failed to calculate Tajima's D for '{vcf_path}': {e}")
        import traceback
        traceback.print_exc() # Print detailed traceback for debugging
        return None

# === Main Script Execution ===

def main():
    print("--- Starting Tajima's D Calculation ---")

    # 1. Get the set of all samples belonging to the target population from the panel file
    target_pop_samples = get_population_samples(PANEL_FILE_LOCAL, POPULATION_CODE)
    if not target_pop_samples:
        print(f"ERROR: Could not retrieve any samples for population {POPULATION_CODE}. Exiting.")
        sys.exit(1)

    # 2. Find all VCF files generated previously
    vcf_pattern = os.path.join(VCF_DIR, "*.vcf.gz")
    vcf_files = glob.glob(vcf_pattern)

    if not vcf_files:
        print(f"ERROR: No VCF files found in directory '{VCF_DIR}' matching pattern '{vcf_pattern}'.")
        print("  Did you run the previous script successfully? Is VCF_DIR set correctly?")
        sys.exit(1)

    print(f"\nFound {len(vcf_files)} VCF files in '{VCF_DIR}'. Processing...")

    results = {} # Dictionary to store results: {vcf_filename: tajima_d_value}

    # 3. Loop through each VCF file
    for vcf_path in sorted(vcf_files): # Sort for consistent order
        print("-" * 40)
        vcf_filename = os.path.basename(vcf_path)
        print(f"Processing file: {vcf_filename}")

        # a. Get samples present in this specific VCF
        samples_in_this_vcf = get_samples_from_vcf(BCFTOOLS_CMD, vcf_path)
        if not samples_in_this_vcf:
            print("Skipping Tajima's D calculation due to issues extracting samples.")
            results[vcf_filename] = "Error (Sample Extraction Failed)"
            continue # Move to the next VCF file

        # b. Find the intersection: which target population samples are in THIS VCF?
        relevant_samples_for_td = target_pop_samples.intersection(samples_in_this_vcf)

        if not relevant_samples_for_td:
            print(f"No {POPULATION_CODE} samples found within this specific VCF file. Cannot calculate Tajima's D.")
            results[vcf_filename] = "No Target Samples Found"
            continue # Move to the next VCF file

        # c. Calculate Tajima's D using the relevant samples for this VCF
        td_value = calculate_tajima_d(vcf_path, relevant_samples_for_td)

        if td_value is not None:
            results[vcf_filename] = f"{td_value:.6f}" # Store as formatted string
        else:
            results[vcf_filename] = "Error (Calculation Failed or No Variation)" # Store error/warning string

    # 4. Print Summary of Results
    print("\n" + "=" * 50)
    print("--- Tajima's D Calculation Summary ---")
    print(f"Population: {POPULATION_CODE}")
    print(f"Processed VCFs from: {VCF_DIR}")
    print("-" * 50)
    if results:
        # Find max length for alignment
        max_len = max(len(f) for f in results.keys())
        print(f"{'VCF File':<{max_len}} | Tajima's D")
        print(f"{'-'*max_len}-|------------")
        for filename, td_result in sorted(results.items()):
             print(f"{filename:<{max_len}} | {td_result}")
    else:
        print("No results generated.")

    print("=" * 50)
    print("--- Script Finished ---")

if __name__ == "__main__":
    main()